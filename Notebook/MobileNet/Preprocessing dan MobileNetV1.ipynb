{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61fae869-c88a-4030-907e-7722538d4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6d520e-584a-4dfc-a872-03ba3e139f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Global & Seed\n",
    "# Path dataset asli\n",
    "ORIGINAL_DATASET = \"D:/KULIAH/SEMESTER 7/Skripsi/Dataset/dataset_trashnet\"\n",
    "\n",
    "# Dataset hasil resize\n",
    "RESIZED_DATASET = \"D:/KULIAH/SEMESTER 7/Skripsi/Dataset/dataset_resize\"\n",
    "\n",
    "# Dataset final (split + augmentasi)\n",
    "FINAL_DATASET = \"D:/KULIAH/SEMESTER 7/Skripsi/Dataset/Dataset_TrashNet_Final\"\n",
    "\n",
    "# Kelas\n",
    "CLASSES = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "\n",
    "# Split ratio\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Image size MobileNet\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5229008-982a-4c80-b15e-12d6fad211a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Resize + Padding (Aspect Ratio Preserved)\n",
    "def resize_with_padding(img, target_size=224):\n",
    "    w, h = img.size\n",
    "    scale = target_size / max(w, h)\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "\n",
    "    img = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "\n",
    "    new_img = Image.new(\"RGB\", (target_size, target_size), (0, 0, 0))\n",
    "    paste_x = (target_size - new_w) // 2\n",
    "    paste_y = (target_size - new_h) // 2\n",
    "    new_img.paste(img, (paste_x, paste_y))\n",
    "\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0943b01e-ec06-4740-944e-5b6dc97cc861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing cardboard: 100%|████████████████████████████████████████████████████████████| 403/403 [00:04<00:00, 92.73it/s]\n",
      "Resizing glass: 100%|████████████████████████████████████████████████████████████████| 501/501 [00:05<00:00, 91.83it/s]\n",
      "Resizing metal: 100%|████████████████████████████████████████████████████████████████| 409/409 [00:04<00:00, 92.15it/s]\n",
      "Resizing paper: 100%|████████████████████████████████████████████████████████████████| 594/594 [00:07<00:00, 83.15it/s]\n",
      "Resizing plastic: 100%|██████████████████████████████████████████████████████████████| 480/480 [00:05<00:00, 89.70it/s]\n",
      "Resizing trash: 100%|████████████████████████████████████████████████████████████████| 137/137 [00:01<00:00, 92.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Resize Seluruh Dataset & Simpan ke Lokal\n",
    "def resize_dataset():\n",
    "    for cls in CLASSES:\n",
    "        src_dir = os.path.join(ORIGINAL_DATASET, cls)\n",
    "        dst_dir = os.path.join(RESIZED_DATASET, cls)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "        for img_name in tqdm(os.listdir(src_dir), desc=f\"Resizing {cls}\"):\n",
    "            img_path = os.path.join(src_dir, img_name)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            resized_img = resize_with_padding(img, IMG_SIZE)\n",
    "            resized_img.save(os.path.join(dst_dir, img_name))\n",
    "\n",
    "resize_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17e09350-99a4-4115-a3c6-af45633f0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat Folder Split Dataset\n",
    "def create_split_dirs():\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in CLASSES:\n",
    "            os.makedirs(os.path.join(FINAL_DATASET, split, cls), exist_ok=True)\n",
    "\n",
    "create_split_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b65363b-c6d6-4750-8720-aab70c9d637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset (70 / 15 / 15)\n",
    "def split_dataset():\n",
    "    for cls in CLASSES:\n",
    "        images = os.listdir(os.path.join(RESIZED_DATASET, cls))\n",
    "        random.shuffle(images)\n",
    "\n",
    "        total = len(images)\n",
    "        train_end = int(total * TRAIN_RATIO)\n",
    "        val_end = train_end + int(total * VAL_RATIO)\n",
    "\n",
    "        for img in images[:train_end]:\n",
    "            shutil.copy(\n",
    "                os.path.join(RESIZED_DATASET, cls, img),\n",
    "                os.path.join(FINAL_DATASET, \"train\", cls, img)\n",
    "            )\n",
    "\n",
    "        for img in images[train_end:val_end]:\n",
    "            shutil.copy(\n",
    "                os.path.join(RESIZED_DATASET, cls, img),\n",
    "                os.path.join(FINAL_DATASET, \"val\", cls, img)\n",
    "            )\n",
    "\n",
    "        for img in images[val_end:]:\n",
    "            shutil.copy(\n",
    "                os.path.join(RESIZED_DATASET, cls, img),\n",
    "                os.path.join(FINAL_DATASET, \"test\", cls, img)\n",
    "            )\n",
    "\n",
    "split_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79376588-7b65-4ad2-af6e-0746dbfb7522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: cardboard\n",
      "Current: 282, Target: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting cardboard: 100%|█████████████████████████████████████████████████████████| 282/282 [00:01<00:00, 211.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentasi selesai untuk cardboard: total 300\n",
      "\n",
      "Class: glass\n",
      "Current: 350, Target: 300\n",
      "→ Tidak perlu augmentasi\n",
      "\n",
      "Class: metal\n",
      "Current: 286, Target: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting metal: 100%|█████████████████████████████████████████████████████████████| 286/286 [00:01<00:00, 270.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentasi selesai untuk metal: total 300\n",
      "\n",
      "Class: paper\n",
      "Current: 415, Target: 300\n",
      "→ Tidak perlu augmentasi\n",
      "\n",
      "Class: plastic\n",
      "Current: 336, Target: 300\n",
      "→ Tidak perlu augmentasi\n",
      "\n",
      "Class: trash\n",
      "Current: 95, Target: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting trash: 100%|███████████████████████████████████████████████████████████████| 95/95 [00:00<00:00, 164.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentasi selesai untuk trash: total 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Augmentasi Offline (Train Only)\n",
    "TARGET_SAMPLES = {\n",
    "    \"cardboard\": 300,\n",
    "    \"glass\": 300,\n",
    "    \"metal\": 300,\n",
    "    \"paper\": 300,\n",
    "    \"plastic\": 300,\n",
    "    \"trash\": 300\n",
    "}\n",
    "\n",
    "def augment_train_data_balanced():\n",
    "    train_dir = os.path.join(FINAL_DATASET, \"train\")\n",
    "\n",
    "    for cls in CLASSES:\n",
    "        cls_path = os.path.join(train_dir, cls)\n",
    "        images = [img for img in os.listdir(cls_path) if not img.startswith(\"aug_\")]\n",
    "        current_count = len(images)\n",
    "        target_count = TARGET_SAMPLES[cls]\n",
    "\n",
    "        print(f\"\\nClass: {cls}\")\n",
    "        print(f\"Current: {current_count}, Target: {target_count}\")\n",
    "\n",
    "        if current_count >= target_count:\n",
    "            print(\"→ Tidak perlu augmentasi\")\n",
    "            continue\n",
    "\n",
    "        aug_needed = target_count - current_count\n",
    "        aug_per_image = math.ceil(aug_needed / current_count)\n",
    "\n",
    "        aug_index = 0\n",
    "\n",
    "        for img_name in tqdm(images, desc=f\"Augmenting {cls}\"):\n",
    "            img_path = os.path.join(cls_path, img_name)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            base, ext = os.path.splitext(img_name)\n",
    "\n",
    "            for i in range(aug_per_image):\n",
    "                if aug_index >= aug_needed:\n",
    "                    break\n",
    "\n",
    "                aug_img = img.copy()\n",
    "\n",
    "                # Pola augmentasi bergilir (aman untuk TrashNet)\n",
    "                mode = i % 4\n",
    "\n",
    "                # 1. Horizontal Flip\n",
    "                if mode == 0:\n",
    "                    aug_img = aug_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "                # 2. Vertical Flip\n",
    "                elif mode == 1:\n",
    "                    aug_img = aug_img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "                # 3. Rotasi kecil ±5°\n",
    "                elif mode == 2:\n",
    "                    aug_img = aug_img.rotate(random.choice([-5, 5]))\n",
    "\n",
    "                # 4. Rotasi kecil + horizontal flip\n",
    "                elif mode == 3:\n",
    "                    aug_img = aug_img.rotate(random.choice([-5, 5]))\n",
    "                    aug_img = aug_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "                aug_filename = f\"aug_{base}_{aug_index}{ext}\"\n",
    "                aug_img.save(os.path.join(cls_path, aug_filename))\n",
    "\n",
    "                aug_index += 1\n",
    "\n",
    "        print(f\"Augmentasi selesai untuk {cls}: total {current_count + aug_index}\")\n",
    "\n",
    "# Jalankan augmentasi\n",
    "augment_train_data_balanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa6940b7-f269-449b-a885-9f493a4badac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2001 images belonging to 6 classes.\n",
      "Found 377 images belonging to 6 classes.\n",
      "Found 383 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Generator (RESCALE 1/255)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "val_test_gen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    os.path.join(FINAL_DATASET, \"train\"),\n",
    "    target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_data = val_test_gen.flow_from_directory(\n",
    "    os.path.join(FINAL_DATASET, \"val\"),\n",
    "    target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "test_data = val_test_gen.flow_from_directory(\n",
    "    os.path.join(FINAL_DATASET, \"test\"),\n",
    "    target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82f6a79b-0027-4a34-aaf5-52c545523704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model MobileNet (Transfer Learning)\n",
    "base_model = MobileNet(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1de5fda3-1d44-49c2-8aa1-be51212f127b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ mobilenet_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,228,864</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ mobilenet_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │       \u001b[38;5;34m3,228,864\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m65,600\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m390\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,294,854</span> (12.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,294,854\u001b[0m (12.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,990</span> (257.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m65,990\u001b[0m (257.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,228,864</span> (12.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,228,864\u001b[0m (12.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build & Compile Model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(len(CLASSES), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85f58f88-2302-4385-8781-fc91b4fc6a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1a8ef9d-dafa-4edb-8711-6d258d04a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 308ms/step - accuracy: 0.3418 - loss: 1.6436 - val_accuracy: 0.5013 - val_loss: 1.3256\n",
      "Epoch 2/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 226ms/step - accuracy: 0.5792 - loss: 1.1956 - val_accuracy: 0.6313 - val_loss: 1.0654\n",
      "Epoch 3/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 225ms/step - accuracy: 0.6742 - loss: 0.9584 - val_accuracy: 0.6711 - val_loss: 0.9138\n",
      "Epoch 4/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 227ms/step - accuracy: 0.7291 - loss: 0.8191 - val_accuracy: 0.7268 - val_loss: 0.8202\n",
      "Epoch 5/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 226ms/step - accuracy: 0.7491 - loss: 0.7350 - val_accuracy: 0.7480 - val_loss: 0.7572\n",
      "Epoch 6/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 227ms/step - accuracy: 0.7751 - loss: 0.6672 - val_accuracy: 0.7480 - val_loss: 0.7070\n",
      "Epoch 7/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 226ms/step - accuracy: 0.7961 - loss: 0.6117 - val_accuracy: 0.7666 - val_loss: 0.6719\n",
      "Epoch 8/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 228ms/step - accuracy: 0.8126 - loss: 0.5719 - val_accuracy: 0.7586 - val_loss: 0.6586\n",
      "Epoch 9/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 226ms/step - accuracy: 0.8301 - loss: 0.5295 - val_accuracy: 0.7613 - val_loss: 0.6291\n",
      "Epoch 10/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 228ms/step - accuracy: 0.8356 - loss: 0.4990 - val_accuracy: 0.7878 - val_loss: 0.6003\n",
      "Epoch 11/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 226ms/step - accuracy: 0.8516 - loss: 0.4676 - val_accuracy: 0.7825 - val_loss: 0.5996\n",
      "Epoch 12/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 233ms/step - accuracy: 0.8611 - loss: 0.4426 - val_accuracy: 0.8143 - val_loss: 0.5682\n",
      "Epoch 13/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 232ms/step - accuracy: 0.8676 - loss: 0.4192 - val_accuracy: 0.8064 - val_loss: 0.5618\n",
      "Epoch 14/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 229ms/step - accuracy: 0.8766 - loss: 0.3954 - val_accuracy: 0.8011 - val_loss: 0.5602\n",
      "Epoch 15/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 226ms/step - accuracy: 0.8936 - loss: 0.3750 - val_accuracy: 0.8196 - val_loss: 0.5339\n",
      "Epoch 16/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 227ms/step - accuracy: 0.8961 - loss: 0.3550 - val_accuracy: 0.8196 - val_loss: 0.5345\n",
      "Epoch 17/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 227ms/step - accuracy: 0.9045 - loss: 0.3380 - val_accuracy: 0.8143 - val_loss: 0.5317\n",
      "Epoch 18/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 228ms/step - accuracy: 0.9115 - loss: 0.3211 - val_accuracy: 0.8064 - val_loss: 0.5254\n",
      "Epoch 19/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 227ms/step - accuracy: 0.9220 - loss: 0.3062 - val_accuracy: 0.8223 - val_loss: 0.5179\n",
      "Epoch 20/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 227ms/step - accuracy: 0.9245 - loss: 0.2908 - val_accuracy: 0.8276 - val_loss: 0.5109\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Total Training Time: 293.63 seconds\n",
      "Average Time per Epoch: 14.68 seconds\n",
      "Training stopped at epoch: 20\n"
     ]
    }
   ],
   "source": [
    "# Training Model + Pengukuran Waktu Komputasi\n",
    "EPOCHS = 20\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_training_time = end_time - start_time\n",
    "epochs_ran = len(history.history[\"loss\"])\n",
    "\n",
    "print(f\"\\nTotal Training Time: {total_training_time:.2f} seconds\")\n",
    "print(f\"Average Time per Epoch: {total_training_time / epochs_ran:.2f} seconds\")\n",
    "print(f\"Training stopped at epoch: {epochs_ran}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b5276af-e57b-4637-b084-6b0c46496af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 321ms/step\n",
      "Total Inference Time (Test Set): 4.3847 seconds\n",
      "Average Inference Time per Image: 0.011448 seconds\n"
     ]
    }
   ],
   "source": [
    "# Waktu Inferensi Keseluruhan Test Set\n",
    "start_time = time.time()\n",
    "\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time_total = end_time - start_time\n",
    "num_samples = test_data.samples\n",
    "\n",
    "print(f\"Total Inference Time (Test Set): {inference_time_total:.4f} seconds\")\n",
    "print(f\"Average Inference Time per Image: {inference_time_total / num_samples:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc199580-0077-463a-b9cb-f2b64507aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 190ms/step - accuracy: 0.7963 - loss: 0.5408\n",
      "Test Accuracy: 0.7963\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi Model (Test Set – Tetap Sama)\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfc8f35f-6de7-4fee-b34d-c122cc4371c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Inference Time (Single Image): 0.057216 seconds\n"
     ]
    }
   ],
   "source": [
    "# Ambil 1 batch\n",
    "x_batch, _ = next(test_data)\n",
    "\n",
    "# Warm-up (penting untuk CNN)\n",
    "_ = model.predict(x_batch[:1])\n",
    "\n",
    "start_time = time.time()\n",
    "_ = model.predict(x_batch[:1])\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Inference Time (Single Image): {(end_time - start_time):.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0eca3e25-a5ae-4395-a23e-4f8b79eacaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 188ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.89      0.79      0.83        61\n",
      "       glass       0.71      0.75      0.73        76\n",
      "       metal       0.84      0.84      0.84        62\n",
      "       paper       0.85      0.90      0.88        90\n",
      "     plastic       0.77      0.75      0.76        72\n",
      "       trash       0.59      0.59      0.59        22\n",
      "\n",
      "    accuracy                           0.80       383\n",
      "   macro avg       0.78      0.77      0.77       383\n",
      "weighted avg       0.80      0.80      0.80       383\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  1  1  8  0  3]\n",
      " [ 0 57  6  2 11  0]\n",
      " [ 2  4 52  3  1  0]\n",
      " [ 4  0  0 81  3  2]\n",
      " [ 0 12  2  0 54  4]\n",
      " [ 0  6  1  1  1 13]]\n"
     ]
    }
   ],
   "source": [
    "# CONFUSION MATRIX & REPORT\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_true = test_data.classes\n",
    "y_pred = np.argmax(model.predict(test_data), axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cbe674a-e0bd-44f5-9bb1-cb29cb25c301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil disimpan di: mobilenetv1_trashnet.keras\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"mobilenetv1_trashnet.keras\"\n",
    "model.save(MODEL_PATH)\n",
    "\n",
    "print(f\"Model berhasil disimpan di: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650013cd-9f86-4a55-b3cb-9b86bb4114fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
