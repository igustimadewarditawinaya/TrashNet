{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029add9e-b6a2-4342-ad99-171880eab239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARY\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120f744a-fbc3-4406-9328-6363d2c608e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETER GLOBAL & SEED\n",
    "FINAL_DATASET = \"D:/KULIAH/SEMESTER 7/Skripsi/Dataset/Dataset_TrashNet_Final\"\n",
    "\n",
    "CLASSES = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eee4f86-bd6d-4da0-bc15-9a9967a73bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2001 images belonging to 6 classes.\n",
      "Found 377 images belonging to 6 classes.\n",
      "Found 383 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA GENERATOR (RESCALE ONLY)\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    os.path.join(FINAL_DATASET, \"train\"),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data = val_test_gen.flow_from_directory(\n",
    "    os.path.join(FINAL_DATASET, \"val\"),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data = val_test_gen.flow_from_directory(\n",
    "    os.path.join(FINAL_DATASET, \"test\"),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c39cef3-9778-48cd-8436-d7b6006a8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL MOBILENETV2\n",
    "base_model = MobileNetV2(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "# Freeze feature extractor\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0dbc68-ca49-4301-8fe4-b2f70226cb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">81,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m81,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m390\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,340,358</span> (8.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,340,358\u001b[0m (8.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,374</span> (321.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m82,374\u001b[0m (321.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build model (APPLE TO APPLE dengan V1)\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(len(CLASSES), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb08c5cb-5f20-4daa-aa9a-1acc116d9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EARLY STOPPING\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d906868-e92c-4364-9c46-43fef68a85b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 263ms/step - accuracy: 0.3783 - loss: 1.6324 - val_accuracy: 0.5836 - val_loss: 1.2298\n",
      "Epoch 2/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 247ms/step - accuracy: 0.6102 - loss: 1.1155 - val_accuracy: 0.6844 - val_loss: 0.9513\n",
      "Epoch 3/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.7011 - loss: 0.8785 - val_accuracy: 0.7135 - val_loss: 0.8225\n",
      "Epoch 4/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 265ms/step - accuracy: 0.7491 - loss: 0.7421 - val_accuracy: 0.7241 - val_loss: 0.7451\n",
      "Epoch 5/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.7766 - loss: 0.6565 - val_accuracy: 0.7480 - val_loss: 0.7086\n",
      "Epoch 6/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 257ms/step - accuracy: 0.8011 - loss: 0.5901 - val_accuracy: 0.7586 - val_loss: 0.6571\n",
      "Epoch 7/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.8246 - loss: 0.5359 - val_accuracy: 0.7692 - val_loss: 0.6422\n",
      "Epoch 8/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 266ms/step - accuracy: 0.8386 - loss: 0.4913 - val_accuracy: 0.7692 - val_loss: 0.6298\n",
      "Epoch 9/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 264ms/step - accuracy: 0.8576 - loss: 0.4539 - val_accuracy: 0.8011 - val_loss: 0.5996\n",
      "Epoch 10/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 263ms/step - accuracy: 0.8746 - loss: 0.4197 - val_accuracy: 0.7931 - val_loss: 0.5843\n",
      "Epoch 11/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 262ms/step - accuracy: 0.8856 - loss: 0.3896 - val_accuracy: 0.8117 - val_loss: 0.5764\n",
      "Epoch 12/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 266ms/step - accuracy: 0.8966 - loss: 0.3660 - val_accuracy: 0.7984 - val_loss: 0.5743\n",
      "Epoch 13/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 252ms/step - accuracy: 0.9070 - loss: 0.3403 - val_accuracy: 0.7984 - val_loss: 0.5491\n",
      "Epoch 14/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 248ms/step - accuracy: 0.9165 - loss: 0.3174 - val_accuracy: 0.7984 - val_loss: 0.5489\n",
      "Epoch 15/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 245ms/step - accuracy: 0.9240 - loss: 0.2989 - val_accuracy: 0.8196 - val_loss: 0.5331\n",
      "Epoch 16/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 252ms/step - accuracy: 0.9315 - loss: 0.2812 - val_accuracy: 0.8117 - val_loss: 0.5293\n",
      "Epoch 17/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.9345 - loss: 0.2655 - val_accuracy: 0.8249 - val_loss: 0.5286\n",
      "Epoch 18/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 263ms/step - accuracy: 0.9455 - loss: 0.2475 - val_accuracy: 0.8143 - val_loss: 0.5238\n",
      "Epoch 19/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.9475 - loss: 0.2321 - val_accuracy: 0.8196 - val_loss: 0.5294\n",
      "Epoch 20/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 263ms/step - accuracy: 0.9530 - loss: 0.2199 - val_accuracy: 0.8249 - val_loss: 0.5231\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Total Training Time : 328.81 seconds\n",
      "Average / Epoch    : 16.44 seconds\n",
      "Training stopped at epoch : 20\n"
     ]
    }
   ],
   "source": [
    "# TRAINING + TIME MEASUREMENT\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_training_time = end_time - start_time\n",
    "epochs_ran = len(history.history[\"loss\"])\n",
    "\n",
    "print(f\"\\nTotal Training Time : {total_training_time:.2f} seconds\")\n",
    "print(f\"Average / Epoch    : {total_training_time / epochs_ran:.2f} seconds\")\n",
    "print(f\"Training stopped at epoch : {epochs_ran}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae0352d5-215a-4dc4-8c75-00849277e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 277ms/step \n",
      "Total Inference Time (Test Set): 4.0858 seconds\n",
      "Average Inference Time per Image: 0.010668 seconds\n"
     ]
    }
   ],
   "source": [
    "# Waktu Inferensi Keseluruhan Test Set\n",
    "start_time = time.time()\n",
    "\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time_total = end_time - start_time\n",
    "num_samples = test_data.samples\n",
    "\n",
    "print(f\"Total Inference Time (Test Set): {inference_time_total:.4f} seconds\")\n",
    "print(f\"Average Inference Time per Image: {inference_time_total / num_samples:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3db4100-b780-4ce9-972f-cdb907376c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - accuracy: 0.8251 - loss: 0.5355\n",
      "Test Accuracy: 0.8251\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi Model (Test Set – Tetap Sama)\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cd4dc4-95cd-4d15-9ae2-d480fdaabad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 665ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Inference Time (Single Image): 0.064460 seconds\n"
     ]
    }
   ],
   "source": [
    "# Ambil 1 batch\n",
    "x_batch, _ = next(test_data)\n",
    "\n",
    "# Warm-up (penting untuk CNN)\n",
    "_ = model.predict(x_batch[:1])\n",
    "\n",
    "start_time = time.time()\n",
    "_ = model.predict(x_batch[:1])\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Inference Time (Single Image): {(end_time - start_time):.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135e67ad-520c-4e47-8699-e171fb213742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.96      0.84      0.89        61\n",
      "       glass       0.78      0.83      0.80        76\n",
      "       metal       0.91      0.85      0.88        62\n",
      "       paper       0.86      0.93      0.89        90\n",
      "     plastic       0.74      0.74      0.74        72\n",
      "       trash       0.57      0.55      0.56        22\n",
      "\n",
      "    accuracy                           0.83       383\n",
      "   macro avg       0.80      0.79      0.79       383\n",
      "weighted avg       0.83      0.83      0.83       383\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[51  1  0  6  1  2]\n",
      " [ 0 63  1  1 10  1]\n",
      " [ 0  2 53  4  3  0]\n",
      " [ 2  0  0 84  2  2]\n",
      " [ 0 12  2  1 53  4]\n",
      " [ 0  3  2  2  3 12]]\n"
     ]
    }
   ],
   "source": [
    "# CONFUSION MATRIX & REPORT\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_true = test_data.classes\n",
    "y_pred = np.argmax(model.predict(test_data), axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67bbc6b5-12be-453d-9f00-d6e1a9076238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model berhasil disimpan di: mobilenetv2_trashnet_PreInput.keras\n"
     ]
    }
   ],
   "source": [
    "# SAVE MODEL (KERAS FORMAT)\n",
    "MODEL_PATH = \"mobilenetv2_trashnet_PreInput.keras\"\n",
    "model.save(MODEL_PATH)\n",
    "\n",
    "print(f\"\\nModel berhasil disimpan di: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd9df0-7977-4ed3-8cb7-6cab6616b6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
