{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f10dd8-b90c-4e81-9f76-8bbfb52081a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tf_pruning_clean\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc91c57-a73b-489c-851e-e11cba06e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_FINE_TUNE = 10\n",
    "\n",
    "BASE_MODEL_PATH = \"D:/KULIAH/SEMESTER 7/Skripsi/Dataset/mobilenetv3Large_trashnet_base\"\n",
    "PRUNED_MODEL_PATH = \"D:/KULIAH/SEMESTER 7/Skripsi/Dataset/mobilenetv3Large_pruned_cek-arsitektur\"\n",
    "\n",
    "DATASET_DIR = \"D:/KULIAH/SEMESTER 7/Skripsi/Dataset/Dataset_TrashNet_Final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7700ec-0aca-47ae-867d-83d6e4c758ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2001 images belonging to 6 classes.\n",
      "Found 377 images belonging to 6 classes.\n",
      "Found 383 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA GENERATOR (RESCALE ONLY)\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    os.path.join(DATASET_DIR, \"train\"),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data = val_test_gen.flow_from_directory(\n",
    "    os.path.join(DATASET_DIR, \"val\"),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data = val_test_gen.flow_from_directory(\n",
    "    os.path.join(DATASET_DIR, \"test\"),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2cf8189-a3c8-4b19-b04f-7fd58d2b7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNGSI CEK UKURAN MODEL\n",
    "def get_model_size_mb(model_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(model_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef344fa-69c0-458d-bb3d-bba7533d7162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tf_pruning_clean\\lib\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\load.py:109: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tf_pruning_clean\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py:156: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tf_pruning_clean\\lib\\site-packages\\tf_keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3large (Function  (None, 7, 7, 960)         2996352   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 960)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                61504     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3058246 (11.67 MB)\n",
      "Trainable params: 61894 (241.77 KB)\n",
      "Non-trainable params: 2996352 (11.43 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.models.load_model(\n",
    "    BASE_MODEL_PATH,\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "base_model.compile(\n",
    "    optimizer=Adam(1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcfad06e-7c52-4467-980e-1d9d839fbaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukuran Base Model      : 16.03 MB\n"
     ]
    }
   ],
   "source": [
    "# CEK UKURAN BASE MODEL\n",
    "base_model_size = get_model_size_mb(BASE_MODEL_PATH)\n",
    "print(f\"Ukuran Base Model      : {base_model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9038fac-2064-4c91-80c7-acf2e69c940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning(model, initial_sparsity, final_sparsity, end_step):\n",
    "\n",
    "    pruning_params = {\n",
    "        \"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=initial_sparsity,\n",
    "            final_sparsity=final_sparsity,\n",
    "            begin_step=0,\n",
    "            end_step=end_step\n",
    "        )\n",
    "    }\n",
    "\n",
    "    def prune_layer(layer):\n",
    "        if isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "            return layer\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "        return layer\n",
    "\n",
    "    return tf.keras.models.clone_model(\n",
    "        model,\n",
    "        clone_function=prune_layer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96731b2b-31df-4634-8fd4-fe74cf729093",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = np.ceil(train_data.samples / BATCH_SIZE)\n",
    "end_step = int(steps_per_epoch * EPOCHS_FINE_TUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e789b336-1e5b-41ab-b9e7-87724993cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tf_pruning_clean\\lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tf_pruning_clean\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "63/63 [==============================] - 67s 535ms/step - loss: 0.1675 - accuracy: 0.9660 - val_loss: 0.4276 - val_accuracy: 0.8488\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 11s 173ms/step - loss: 0.1512 - accuracy: 0.9710 - val_loss: 0.4196 - val_accuracy: 0.8568\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 11s 175ms/step - loss: 0.1408 - accuracy: 0.9740 - val_loss: 0.4178 - val_accuracy: 0.8594\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 11s 171ms/step - loss: 0.1304 - accuracy: 0.9780 - val_loss: 0.4156 - val_accuracy: 0.8568\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 11s 171ms/step - loss: 0.1203 - accuracy: 0.9815 - val_loss: 0.4048 - val_accuracy: 0.8568\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 11s 172ms/step - loss: 0.1118 - accuracy: 0.9835 - val_loss: 0.3998 - val_accuracy: 0.8621\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 11s 170ms/step - loss: 0.1033 - accuracy: 0.9835 - val_loss: 0.4131 - val_accuracy: 0.8488\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 11s 173ms/step - loss: 0.0970 - accuracy: 0.9875 - val_loss: 0.3953 - val_accuracy: 0.8541\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 11s 171ms/step - loss: 0.0910 - accuracy: 0.9895 - val_loss: 0.4039 - val_accuracy: 0.8568\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 11s 174ms/step - loss: 0.0848 - accuracy: 0.9920 - val_loss: 0.3982 - val_accuracy: 0.8541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x2a37d05ded0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model = apply_pruning(base_model, 0.0, 0.20, end_step)\n",
    "\n",
    "pruned_model.compile(\n",
    "    optimizer=Adam(1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    EarlyStopping(patience=2, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "pruned_model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS_FINE_TUNE,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b105efd7-271b-43ff-ac96-2abd378df661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 32s 282ms/step - loss: 0.0936 - accuracy: 0.9875 - val_loss: 0.3987 - val_accuracy: 0.8541\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 11s 168ms/step - loss: 0.0838 - accuracy: 0.9910 - val_loss: 0.4031 - val_accuracy: 0.8594\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 11s 170ms/step - loss: 0.0778 - accuracy: 0.9925 - val_loss: 0.3964 - val_accuracy: 0.8568\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 11s 170ms/step - loss: 0.0723 - accuracy: 0.9955 - val_loss: 0.3963 - val_accuracy: 0.8568\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 11s 170ms/step - loss: 0.0669 - accuracy: 0.9965 - val_loss: 0.4006 - val_accuracy: 0.8568\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 11s 170ms/step - loss: 0.0616 - accuracy: 0.9970 - val_loss: 0.4036 - val_accuracy: 0.8594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x2a3001861d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model = apply_pruning(pruned_model, 0.20, 0.30, end_step)\n",
    "\n",
    "pruned_model.compile(\n",
    "    optimizer=Adam(1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "pruned_model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS_FINE_TUNE,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d23a00db-9a0f-403c-b540-f364ff080b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 33s 293ms/step - loss: 0.0687 - accuracy: 0.9945 - val_loss: 0.3883 - val_accuracy: 0.8647\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 11s 167ms/step - loss: 0.0620 - accuracy: 0.9965 - val_loss: 0.3954 - val_accuracy: 0.8568\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 11s 170ms/step - loss: 0.0571 - accuracy: 0.9975 - val_loss: 0.3948 - val_accuracy: 0.8621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x2a300743700>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model = apply_pruning(pruned_model, 0.30, 0.40, end_step)\n",
    "\n",
    "pruned_model.compile(\n",
    "    optimizer=Adam(1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "pruned_model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS_FINE_TUNE,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c30114-3e7c-4d8c-aa3c-73ce8aba5bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ringkasan Arsitektur Model Setelah Pruning (Sebelum Strip)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3large (Function  (None, 7, 7, 960)         2996352   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 960)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                61504     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3058246 (11.67 MB)\n",
      "Trainable params: 61894 (241.77 KB)\n",
      "Non-trainable params: 2996352 (11.43 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRingkasan Arsitektur Model Setelah Pruning (Sebelum Strip)\")\n",
    "pruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d3d1264-c825-47c5-88d0-3775fb03996f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer yang Terkena Pruning Wrapper\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLayer yang Terkena Pruning Wrapper\")\n",
    "for layer in pruned_model.layers:\n",
    "    if hasattr(layer, \"pruning_step\"):\n",
    "        print(f\"Layer dipruning: {layer.layer.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c728757-cbc8-4637-8ae3-25a35ae9da50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Persentase Bobot Nol per Layer (Sebelum Strip)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPersentase Bobot Nol per Layer (Sebelum Strip)\")\n",
    "\n",
    "for layer in pruned_model.layers:\n",
    "    if hasattr(layer, \"layer\"):  # pruning wrapper\n",
    "        weights = layer.layer.get_weights()\n",
    "        if len(weights) > 0:\n",
    "            w = weights[0]\n",
    "            zero_ratio = np.sum(w == 0) / w.size * 100\n",
    "            print(f\"{layer.layer.name}: {zero_ratio:.2f}% bobot nol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d3996d6-0aa3-4b05-b545-80c52a9cf925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Conv2D yang Sepenuhnya Nol\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFilter Conv2D yang Sepenuhnya Nol\")\n",
    "\n",
    "for layer in pruned_model.layers:\n",
    "    if hasattr(layer, \"layer\") and isinstance(layer.layer, tf.keras.layers.Conv2D):\n",
    "        w = layer.layer.get_weights()[0]\n",
    "        zero_filters = np.sum(np.all(w == 0, axis=(0, 1, 2)))\n",
    "        total_filters = w.shape[-1]\n",
    "        print(f\"{layer.layer.name}: {zero_filters}/{total_filters} filter nol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c83c9ece-26af-437e-8ea3-77ef497454fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ab6a35-69bb-4de4-b5b5-be32d0f16951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: D:/KULIAH/SEMESTER 7/Skripsi/Dataset/mobilenetv3Large_pruned_cek-arsitektur\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/KULIAH/SEMESTER 7/Skripsi/Dataset/mobilenetv3Large_pruned_cek-arsitektur\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pruning berhasil disimpan\n"
     ]
    }
   ],
   "source": [
    "final_pruned_model.save(\n",
    "    PRUNED_MODEL_PATH,\n",
    "    save_format=\"tf\"\n",
    ")\n",
    "\n",
    "print(\"Model pruning berhasil disimpan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a52b9fb-d5f3-445a-90de-76426447eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukuran Pruned Model    : 15.55 MB\n",
      "Compression Ratio     : 1.03x\n",
      "Size Reduction        : 2.94%\n"
     ]
    }
   ],
   "source": [
    "# CEK UKURAN MODEL SETELAH PRUNING\n",
    "pruned_model_size = get_model_size_mb(PRUNED_MODEL_PATH)\n",
    "print(f\"Ukuran Pruned Model    : {pruned_model_size:.2f} MB\")\n",
    "\n",
    "compression_ratio = base_model_size / pruned_model_size\n",
    "size_reduction = (1 - (pruned_model_size / base_model_size)) * 100\n",
    "\n",
    "print(f\"Compression Ratio     : {compression_ratio:.2f}x\")\n",
    "print(f\"Size Reduction        : {size_reduction:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48c798a4-d51f-41a5-97da-5ac0c9a3a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 11s 364ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89        61\n",
      "           1       0.84      0.91      0.87        76\n",
      "           2       0.90      0.85      0.88        62\n",
      "           3       0.88      0.90      0.89        90\n",
      "           4       0.79      0.85      0.82        72\n",
      "           5       0.82      0.64      0.72        22\n",
      "\n",
      "    accuracy                           0.86       383\n",
      "   macro avg       0.86      0.83      0.84       383\n",
      "weighted avg       0.86      0.86      0.86       383\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[52  0  0  8  0  1]\n",
      " [ 0 69  2  0  5  0]\n",
      " [ 0  3 53  3  3  0]\n",
      " [ 4  0  0 81  4  1]\n",
      " [ 0  7  3  0 61  1]\n",
      " [ 0  3  1  0  4 14]]\n"
     ]
    }
   ],
   "source": [
    "test_data.reset()\n",
    "\n",
    "predictions = final_pruned_model.predict(test_data)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_data.classes\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66bd9875-b3b3-479a-951c-5e60b1292fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def benchmark_inference(model, data_gen, warmup=3, runs=10):\n",
    "    \"\"\"\n",
    "    model      : stripped pruned model\n",
    "    data_gen   : test_data (shuffle=False)\n",
    "    warmup     : jumlah warmup run\n",
    "    runs       : jumlah benchmark run\n",
    "    \"\"\"\n",
    "\n",
    "    # Ambil 1 batch data\n",
    "    x_batch, _ = next(data_gen)\n",
    "\n",
    "    # Warm-up (penting untuk TensorFlow)\n",
    "    for _ in range(warmup):\n",
    "        _ = model.predict(x_batch, verbose=0)\n",
    "\n",
    "    times = []\n",
    "\n",
    "    for _ in range(runs):\n",
    "        start = time.time()\n",
    "        _ = model.predict(x_batch, verbose=0)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "\n",
    "    avg_time = np.mean(times)\n",
    "    per_image_time = avg_time / x_batch.shape[0]\n",
    "\n",
    "    print(\"Inference Benchmark\")\n",
    "    print(f\"Batch size         : {x_batch.shape[0]}\")\n",
    "    print(f\"Avg batch time     : {avg_time:.4f} seconds\")\n",
    "    print(f\"Avg per image time : {per_image_time:.6f} seconds\")\n",
    "\n",
    "    return avg_time, per_image_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf63382d-cdf2-473c-a9e3-fbb380a863c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Benchmark\n",
      "Batch size         : 32\n",
      "Avg batch time     : 0.1857 seconds\n",
      "Avg per image time : 0.005804 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.18573572635650634, 0.005804241448640823)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_inference(final_pruned_model, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_pruning_clean)",
   "language": "python",
   "name": "tf_pruning_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
